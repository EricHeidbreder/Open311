{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from auth_script import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://test311api.cityofchicago.org/open311/v2/\"\n",
    "services = base_url + 'services.json'\n",
    "service_requests = base_url + 'requests.json'\n",
    "s = requests.Session()\n",
    "\n",
    "# building initial dataframe structure\n",
    "service_requests_df = pd.DataFrame(columns=['service_request_id', 'status', 'service_name', 'service_code',\n",
    "       'requested_datetime', 'updated_datetime', 'address', 'lat', 'long',\n",
    "       'extended_attributes', 'notes'])\n",
    "\n",
    "# Set up initial headers\n",
    "page_num = 3501\n",
    "service_params = {\n",
    "    'page': page_num,\n",
    "    'page_size': 200,\n",
    "    'extensions': 'true'\n",
    "}\n",
    "service_requests_payload = s.get(service_requests, params=service_params).json()\n",
    "results = []\n",
    "\n",
    "# Get all data from REST API\n",
    "while len(service_requests_payload) == 200:\n",
    "    # store the current payload in a list\n",
    "    results.extend(service_requests_payload)\n",
    "\n",
    "    # update the page number\n",
    "    page_num += 1\n",
    "    service_params['page'] = page_num\n",
    "\n",
    "    try:\n",
    "        # generate a new payload\n",
    "        service_requests_payload = s.get(service_requests, params=service_params).json()\n",
    "    except:\n",
    "        s = requests.Session()\n",
    "        service_requests_payload = s.get(service_requests, params=service_params).json()\n",
    "\n",
    "    if page_num % 10 == 0:\n",
    "        print(f'Current Page: {page_num}')\n",
    "    \n",
    "    if page_num % 500 == 0:\n",
    "        json.dump(results, open(f'./temp/service_requests_{page_num-500}-{page_num}.json', 'w'), ensure_ascii=False, indent=4)\n",
    "        results = []\n",
    "# Once finished with the loop, we'll need to write the last bit of data\n",
    "json.dump(results, open(f'./temp/service_requests_{page_num}.json', 'w'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json(temp_path, new_path, new_file_name):\n",
    "    results = []\n",
    "    for f in os.listdir(temp_path):\n",
    "        with open(f'./temp/{f}') as infile:\n",
    "            results.extend(json.load(infile))\n",
    "    with open(f'./data/{new_file_name}', 'w+') as outfile:\n",
    "        json.dump(results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_json('./temp/', './data/', 'service_requests_full.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get services list\n",
    "services_list = requests.get(services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create services df\n",
    "services_list_df = pd.json_normalize(services_list.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_list(df, list_column, new_column): \n",
    "    lens_of_lists = df[list_column].apply(len)\n",
    "    origin_rows = range(df.shape[0])\n",
    "    destination_rows = np.repeat(origin_rows, lens_of_lists)\n",
    "    non_list_cols = (\n",
    "      [idx for idx, col in enumerate(df.columns)\n",
    "       if col != list_column]\n",
    "    )\n",
    "    expanded_df = df.iloc[destination_rows, non_list_cols].copy()\n",
    "    expanded_df[new_column] = (\n",
    "      [item for items in df[list_column] for item in items]\n",
    "      )\n",
    "    expanded_df.reset_index(inplace=True, drop=True)\n",
    "    return expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# load json data\n",
    "sr_df = pd.json_normalize(json.load(open('./data/service_requests_full.json', 'r')))\n",
    "\n",
    "# remove duplicates from errant data gathering\n",
    "sr_df.drop_duplicates('service_request_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the np.nans in the notes column because they were throwing an error\n",
    "sr_df.loc[sr_df['notes'].isna(), 'notes'] = sr_df.loc[sr_df['notes'].isna(), 'notes'].apply(lambda x: [{'NA': 'NA'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# expanding the notes section out to individual rows. This data frame now has 'per action' granularity\n",
    "sr_notes_df = pd.json_normalize(expand_list(sr_df, 'notes', 'notes_indiv')['notes_indiv'])\n",
    "\n",
    "# Ensuring that the work orders are attached to each row\n",
    "sr_notes_df['extended_attributes.service_request_number'] = sr_notes_df['extended_attributes.service_request_number'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the two dfs together, though I can probably keep these separate and join them in Qlik\n",
    "sr_notes_concat_df = pd.concat([expand_list(sr_df, 'notes', 'notes_indiv'), sr_notes_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services_list_df.to_csv('./data/services.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_df.to_csv('./data/service_requests.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_notes_df.to_csv('./data/service_requests_notes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_notes_concat_df.to_csv('./data/service_requests_notes_concat.csv', index=False)"
   ]
  }
 ]
}